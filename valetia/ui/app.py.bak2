"""
Module de conversation intelligente pour Valetia.
Utilise transformers pour la compréhension et génération de texte.
"""

import os
from pathlib import Path
import time
from typing import Dict, List, Optional, Tuple, Union

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import chromadb
from chromadb.config import Settings

from valetia.utils.logger import get_logger

logger = get_logger(__name__)

class ConversationManager:
    """Gère les conversations avec l'utilisateur."""
    
    def __init__(self, model_name: str = "distilgpt2"):
        """
        Initialise le gestionnaire de conversation.
        
        Args:
            model_name: Nom du modèle à charger. Par défaut "distilgpt2" (léger).
                        Pour une meilleure qualité, utiliser "gpt2" ou "facebook/blenderbot-400M-distill".
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Initialisation du ConversationManager avec le modèle {model_name} sur {self.device}")
        
        # Chargement du modèle et du tokenizer
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
            self.model.to(self.device)
            logger.info(f"Modèle {model_name} chargé avec succès")
        except Exception as e:
            logger.error(f"Erreur lors du chargement du modèle {model_name}: {e}")
            # Fallback sur un modèle plus léger en cas d'erreur
            self.model_name = "distilgpt2"
            self.tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
            self.model = AutoModelForCausalLM.from_pretrained("distilgpt2")
            self.model.to(self.device)
        
        # Initialisation de la base de données vectorielle pour stocker l'historique
        self.db_path = Path("data/conversations")
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        self.db = chromadb.PersistentClient(
            path=str(self.db_path),
            settings=Settings(allow_reset=True)
        )
        
        # Création de la collection pour stocker les conversations
        try:
            self.collection = self.db.get_or_create_collection(name="conversations")
            logger.info("Collection ChromaDB 'conversations' initialisée")
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation de ChromaDB: {e}")
    
    def get_response(self, 
                     user_input: str, 
                     conversation_id: str, 
                     context: Optional[List[Dict[str, str]]] = None) -> str:
        """
        Génère une réponse à partir de l'entrée utilisateur.
        
        Args:
            user_input: Texte envoyé par l'utilisateur
            conversation_id: Identifiant unique de la conversation
            context: Contexte optionnel (métadonnées sur le dossier)
            
        Returns:
            Réponse générée
        """
        # Récupérer l'historique de la conversation
        history = self._get_conversation_history(conversation_id)
        
        # Construction du prompt avec l'historique
        prompt = self._build_prompt(user_input, history)
        
        # Génération de la réponse
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            outputs = self.model.generate(
                inputs.input_ids,
                max_length=150,
                num_return_sequences=1,
                temperature=0.7,
                top_k=50,
                top_p=0.95,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extraire seulement la réponse sans répéter le prompt
            response = response[len(prompt):].strip()
            
            # Fallback si la réponse est vide
            if not response:
                response = "Je ne suis pas sûr de comprendre. Pourriez-vous reformuler ou préciser votre question?"
        except Exception as e:
            logger.error(f"Erreur lors de la génération de la réponse: {e}")
            response = "Désolé, j'ai rencontré un problème technique. Veuillez réessayer."
        
        # Enregistrer l'échange dans l'historique
        self._save_conversation(conversation_id, user_input, response, context)
        
        return response
    
    def _build_prompt(self, user_input: str, history: List[Dict[str, str]]) -> str:
        """Construit le prompt avec l'historique de la conversation."""
        prompt = ""
        
        # Inclure l'historique récent dans le prompt (limité aux 5 derniers échanges)
        for exchange in history[-5:]:
            prompt += f"Utilisateur: {exchange['user']}\nAssistant: {exchange['assistant']}\n"
        
        # Ajouter la question actuelle
        prompt += f"Utilisateur: {user_input}\nAssistant:"
        
        return prompt
    
    def _get_conversation_history(self, conversation_id: str) -> List[Dict[str, str]]:
        """Récupère l'historique de la conversation depuis ChromaDB."""
        try:
            results = self.collection.get(
                where={"conversation_id": conversation_id},
                limit=10  # Limiter aux 10 derniers échanges
            )
            
            if not results or not results['metadatas']:
                return []
            
            # Trier par timestamp
            exchanges = [(metadata, metadata.get('timestamp', 0)) 
                        for metadata in results['metadatas']]
            exchanges.sort(key=lambda x: x[1])
            
            history = []
            for exchange, _ in exchanges:
                if 'user_input' in exchange and 'assistant_response' in exchange:
                    history.append({
                        'user': exchange['user_input'],
                        'assistant': exchange['assistant_response']
                    })
            
            return history
        except Exception as e:
            logger.error(f"Erreur lors de la récupération de l'historique: {e}")
            return []
    
    def _save_conversation(self, 
                          conversation_id: str, 
                          user_input: str, 
                          assistant_response: str,
                          context: Optional[List[Dict[str, str]]] = None) -> None:
        """Enregistre l'échange dans ChromaDB."""
        try:
            timestamp = int(time.time())
            exchange_id = f"{conversation_id}_{timestamp}"
            
            metadata = {
                "conversation_id": conversation_id,
                "user_input": user_input,
                "assistant_response": assistant_response,
                "timestamp": timestamp
            }
            
            # Ajouter le contexte aux métadonnées s'il est fourni
            if context:
                for i, ctx in enumerate(context):
                    for key, value in ctx.items():
                        metadata[f"context_{i}_{key}"] = str(value)
            
            # Créer un embeddings (simplifiée pour l'instant)
            # TODO: Utiliser un modèle d'embeddings plus sophistiqué
            combined_text = f"{user_input} {assistant_response}"
            
            self.collection.add(
                ids=[exchange_id],
                documents=[combined_text],
                metadatas=[metadata]
            )
            
            logger.info(f"Conversation {conversation_id} mise à jour")
        except Exception as e:
            logger.error(f"Erreur lors de l'enregistrement de la conversation: {e}")


# Exemple d'utilisation:
if __name__ == "__main__":
    # Test simple
    conversation_manager = ConversationManager()
    response = conversation_manager.get_response(
        "Bonjour, comment puis-je analyser un document de copropriété?",
        "test_conversation_123"
    )
    print(f"Réponse: {response}")
